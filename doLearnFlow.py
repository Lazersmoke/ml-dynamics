import os
# Tell tensorflow not to use the GPU
#os.environ["CUDA_VISIBLE_DEVICES"] = "-1"
# Tone down warning messages
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'

print("Loading TensorFlow...")
import tensorflow as tf

from tensorflow.keras.layers import Dense, Flatten, Conv2D, Dropout
from tensorflow.keras import Model

import random
import numpy as np
import scipy.io as sio
import matplotlib.pyplot as plt
plt.rc('image', cmap='inferno')

import matplotlib.colors as mplcolors
from scipy import ndimage, misc, signal, stats

### Settings ###
EPOCHS = 100

# Make low modes curve for charge count?
# May be bad for large data
doFreqz = False

# Take only states with a fixed number of singularities, or (-1) for all states
fixedChargeCount = 12

# Number of FFT modes to keep (beyond zero)
fftKeep = 9
fftDiam = 2 * fftKeep + 1

# True -> Randomly half are test/train
# False -> First half are train, second half are test
doSharding = True
batchSize = 64

# Whether or not to use this laminar-ignoring training method
freeLaminar = False

# Mean flow centering
meanFlowCorrection = True

# Use the IFFT function in real space as the loss function
realSpaceLossFn = False
### /Settings ###

# Import training data from MATLAB file
# Generated by makeStraems.m
fname = "trainingData10000.mat"
print("Reading " + fname + " ...")
trainingData = sio.loadmat(fname)
psiOutsAll = trainingData['psiOuts']
featureOutsAll = trainingData['featureOuts']

spaceSize = psiOutsAll.shape[-1]
spaceCenter = spaceSize // 2

data_frameTime = trainingData['timeBetweenFrames'][0,0]
data_npu = trainingData['npu'][0,0]
data_maxDefects = trainingData['maxDefects'][0,0]
data_dissipations = trainingData['dissipations']
chargeCount = np.squeeze(trainingData['chargeCount'])

print(
  f'Training data metadata:\n'
  f'  frameTime: {data_frameTime}\n'
  f'  npu: {data_npu}\n'
  f'  maxDefects allowed: {data_maxDefects}\n'
  f'  numDataPoints: {psiOutsAll.shape[0]}\n'
  )
plt.figure(figsize=(18,7))
xrng = np.arange(np.size(chargeCount))

if doFreqz:
  freqz = np.fft.rfft(chargeCount)
  rfftLen = np.size(chargeCount) // 2 + 1
  cutModes = 10
  normedFreqz = freqz/(1 + np.arange(rfftLen))
  argSorted = np.argsort(np.abs(normedFreqz))
  maxWave = tf.math.divide_no_nan(data_frameTime * np.size(chargeCount) * 2.0,argSorted[-cutModes:])
  maxFreqAbs = np.abs(normedFreqz[argSorted[-cutModes:]])/np.size(chargeCount)
  # Output the prominent wavelengths
  # If the charge count looks periodic, it should show up here
  print(f'Prominent wavelengths in charge count: {maxWave}\nWith contributions: {maxFreqAbs}')
  freqz[argSorted[:-cutModes]] = 0
  fftChargeCount = np.fft.irfft(freqz)
  plt.plot(data_frameTime * xrng,fftChargeCount,label='Charge count, low modes')

# Plot charge count and dissipation vs time
plt.scatter(data_frameTime * xrng,chargeCount,marker = ',',s=1,label='Charge count')
plt.plot(data_frameTime * xrng,np.mean(chargeCount) + np.std(chargeCount) * stats.zscore(data_dissipations),'o-',color='xkcd:mulberry',label='Dissipation')
plt.xlim(1,1000)
plt.title("Charge count vs time")
plt.legend()
plt.show(block=True)

if fixedChargeCount > -1:
  psiOuts = psiOutsAll[chargeCount == fixedChargeCount]
  featureOuts = featureOutsAll[chargeCount == fixedChargeCount,:12]
if fixedChargeCount == -1:
  maxCharge = np.amax(chargeCount)
  psiOuts = psiOutsAll
  featureOuts = featureOutsAll[:,:maxCharge]

# Create the k^3 scaling data
absScaling = fftDiam * (fftDiam // 2 + 1) * 500
fftScalingTable = tf.convert_to_tensor(np.ndarray.flatten(np.fromfunction(lambda x,y: (((x - fftKeep) ** 2 + y ** 2) ** (3/2))/absScaling,[fftDiam,fftKeep + 1])),tf.complex64)

# Goes like k^{-3}
fftUnscalingTable = tf.math.divide_no_nan(tf.cast(tf.constant(1),tf.complex64),fftScalingTable)

def getSmallFFT(psi):
  fftd = tf.cast(tf.signal.fftshift(tf.signal.rfft2d(psi),axes = -2)[...,spaceCenter - fftKeep : spaceCenter + fftKeep + 1,:fftKeep + 1],tf.complex64)
  return fftScalingTable * tf.reshape(fftd,[fftd.shape[0],fftd.shape[-1] * fftd.shape[-2]])


# Undo the flattening to get a nice plot
def toFFTSq(label):
  return np.reshape(label,[fftDiam,fftKeep + 1])

forcingWiggle = np.fromfunction(lambda x,y: y + 2 * np.sin(4 * 2 * np.math.pi/128 * x),[128,128])
def visualize(trueLabel,recons,original,lab="",epc=""):
  # Give it the laminar part for free!
  if freeLaminar:
    recons = recons - laminarFFTProfile * (recons - trueLabel)

  # Take the mean flow out of the label
  if meanFlowCorrection:
    recons = recons
    trueLabel = trueLabel - meanFlowFFT

  # FFT plot
  f, (ax1, ax2, ax3) = plt.subplots(1, 3,figsize=(30,10))
  ax1.set_title("FFT abs reconstructed (" + lab + ")" + (" using freeLaminar" if freeLaminar else ""))
  maxVal = np.amax(np.abs(trueLabel))
  f.colorbar(ax1.imshow(np.abs(toFFTSq(recons)), cmap='Blues', vmin = 0, vmax = maxVal, interpolation='none',extent = [0,fftKeep,fftKeep,-fftKeep]),ax = ax1)

  ax2.set_title("FFT abs True (" + lab + ")" + (" sans mean flow" if meanFlowCorrection else ""))
  f.colorbar(ax2.imshow(np.abs(toFFTSq(trueLabel)), cmap='Blues', vmin = 0, vmax = maxVal, interpolation='none',extent = [0,fftKeep,fftKeep,-fftKeep]),ax = ax2)

  lossAmt = lossFn(trueLabel,recons).numpy()
  ax3.set_title("Errors (" + lab + "), loss = " + str(lossAmt) + ", epoch #" + epc)
  f.colorbar(ax3.imshow(np.abs(toFFTSq(recons - trueLabel)), cmap='Reds', interpolation='none'),ax = ax3)

  f.savefig("training_data/fft" + lab + ".png")
  plt.close(f)

  # Real space plot
  f, (ax1) = plt.subplots(1, 1,figsize=(10,10))
  if meanFlowCorrection:
    bandLimitPsi = tfUnLabel(trueLabel + meanFlowFFT)
    psiRecon = tfUnLabel(recons + meanFlowFFT)
  else:
    bandLimitPsi = tfUnLabel(trueLabel)
    psiRecon = tfUnLabel(recons)
  
  ax1.set_title("Recovered vs FFT Bandlimited Original (" + lab + "), loss = " + str(tf.reduce_mean(tf.keras.losses.mae(original,psiRecon)).numpy()) + ", epoch #" + epc)

  ax1.contourf(bandLimitPsi,cmap="gray",levels=20)
  ax1.contour(original,cmap="gray",levels=20)

  ax1.contour(psiRecon,cmap="inferno",levels=20)

  ax1.contour(forcingWiggle,colors='blue',alpha=0.05,levels=16)
  f.savefig("training_data/current" + lab + ".png")
  plt.close(f)

  bandLimitPsi.numpy().astype('float32').tofile("training_data/reconOrig" + lab + '.dat')
  psiRecon.numpy().astype('float32').tofile("training_data/recon" + lab + '.dat')

# Work in float32 precision
workedFeatures = tf.cast(featureOuts,tf.float32)

# This should correctly obey the fixedChargeCount setting
print("Example input feature:")
print(workedFeatures[0])

psiFFTs = getSmallFFT(psiOuts)
#print("Example of FFT outs:")
#print(psiFFTs[0])
#print(np.abs(psiFFTs[0]))
print("Features (defects) shape: ",workedFeatures.shape)
print("Labels (small FFTs) shape: ",psiFFTs.shape)

if doSharding:
  whole_ds = tf.data.Dataset.from_tensor_slices((workedFeatures,psiFFTs,psiOuts)).shuffle(10000).batch(batchSize)
  train_ds = whole_ds.shard(2,0)
  test_ds = whole_ds.shard(2,1)

if not doSharding:
  trainTestSplit = psiOuts.shape[0] // 2

  x_train = workedFeatures[:trainTestSplit]
  y_train = psiFFTs[:trainTestSplit]
  q_train = psiOuts[:trainTestSplit]

  x_test = workedFeatures[trainTestSplit:takeExamples]
  y_test = psiFFTs[trainTestSplit:takeExamples]
  q_test = psiOuts[trainTestSplit:takeExamples]

  train_ds = tf.data.Dataset.from_tensor_slices(
      (x_train, y_train, q_train)).shuffle(10000).batch(batchSize)

  test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test, q_test)).shuffle(10000).batch(batchSize)

class DirectModel(Model):
  def __init__(self):
    super(DirectModel, self).__init__()
    self.outDim = fftDiam * (fftKeep + 1)
    self.d1 = Dense(32, activation='relu',input_shape=(32,7))
    self.drop = Dropout(0.2)
    self.d2 = Dense(self.outDim * 2)
    self.flat = Flatten()
    self.outd = Dense(self.outDim * 2)

  def call(self, x):
    x = self.flat(x)
    x = self.d1(x)
    x = self.drop(x) # Prevents overfitting
    x = self.d2(x)
    x = tf.cast(self.outd(x),tf.complex64)
    x = x[...,:self.outDim] + 1j * x[...,self.outDim:]
    return self.flat(x)

model = DirectModel()

# Do an IFFT on a truncated FFT
# tf because it's differentiable, and can be used as a loss function
def tfUnLabel(smallFFT):
  effBatchSize = tf.shape(smallFFT)[:-1]
  finalShape = tf.concat([effBatchSize , [fftDiam,fftKeep + 1]],0)
  padCmplx = [(spaceSize // 2) - fftKeep,(spaceSize // 2) - fftKeep - 1]
  padRe = [0,(spaceSize // 2) - (fftKeep + 1)]
  reScaled = tf.reshape(smallFFT * fftUnscalingTable,finalShape)
  paddings = tf.concat([tf.zeros([tf.size(effBatchSize),2],tf.int32),[padCmplx,padRe]],0)
  padded = tf.pad(reScaled,paddings)
  rolled = tf.signal.ifftshift(padded,axes=-2)
  return tf.signal.irfft2d(rolled,fft_length=[128,128])

# tfUnLabel, but with a statically known batch size. Should be faster
def tfUnLabelFast(bs,smallFFT):
  finalShape = [bs,fftDiam,fftKeep + 1]
  padCmplx = [(spaceSize // 2) - fftKeep,(spaceSize // 2) - fftKeep - 1]
  padRe = [0,(spaceSize // 2) - (fftKeep + 1)]
  reScaled = tf.reshape(smallFFT * fftUnscalingTable,finalShape)
  paddings = [[0,0],padCmplx,padRe]
  padded = tf.pad(reScaled,paddings)
  rolled = tf.signal.ifftshift(padded,axes=-2)
  return tf.signal.irfft2d(rolled,fft_length=[128,128])


# Mean flow correction
#meanFlowReal = np.reshape(np.fromfile("noisyLaminarFlow.dat","float32"),[1,128,128])
#meanFlowReal = np.mean(psiOutsAll,axis=0)

if meanFlowCorrection:
  #meanFlowFFT = tf.squeeze(getSmallFFT(meanFlowReal))
  meanFlowFFT = tf.math.reduce_mean(getSmallFFT(psiOutsAll),axis=0)
else:
  meanFlowFFT = tf.zeros_like(fftScalingTable)

# This creates an array highlighting our expected Laminar flow FFT wavenumbers:
# ...
# 0000000
# 1111100
# 0000000
# 0000000
# 0000000
#=0000000... <- Freq zero row
# 0000000
# 0000000
# 0000000
# 1111100
# 0000000
# ...
kf = 4
forcingModesPerp = 5

laminarFFTIdx = np.zeros([fftDiam,fftKeep + 1])
laminarFFTIdx[fftKeep - kf,0:forcingModesPerp] = 1
laminarFFTIdx[fftKeep + kf,0:forcingModesPerp] = 1
# complex64 and flat so we can multiply with an FFT label
laminarFFTProfile = tf.cast(tf.convert_to_tensor(np.ndarray.flatten(laminarFFTIdx)),tf.complex64)

# Makes a laminar flow real space state with phase zero
tfUnLabel(laminarFFTProfile).numpy().astype('float32').tofile("laminarProfile.dat")

# This is the actual mask we use in the loss function
nonLaminarFFTProfile = tf.constant(1,tf.complex64) - laminarFFTProfile

def lossOnIFFT(actualSmallFFT, predictedSmallFFT):
  bs = tf.shape(actualSmallFFT)[0]
  return tf.keras.losses.mse(tfUnLabelFast(bs,actualSmallFFT),tfUnLabelFast(bs,predictedSmallFFT))

def lossFn(actualSmallFFT, predictedSmallFFT):
  actualSmallFFT = actualSmallFFT - meanFlowFFT
  errors = tf.math.log(1 + tf.abs(actualSmallFFT - predictedSmallFFT))
  sqErrs = tf.math.log(tf.math.cosh(errors))
  return tf.math.reduce_mean(sqErrs,axis = -1)

def lossFnLaminarCorrected(actualSmallFFT, predictedSmallFFT):
  actualSmallFFT = nonLaminarFFTProfile * actualSmallFFT
  predictedSmallFFT = nonLaminarFFTProfile * predictedSmallFFT
  return lossFn(actualSmallFFT,predictedSmallFFT)

optimizer = tf.keras.optimizers.Adam()

train_loss = tf.keras.metrics.Mean(name='train_loss')

test_loss = tf.keras.metrics.Mean(name='test_loss')

@tf.function
def train_step(features, labels):
  # Randomize defects in list to remove any bias from intersections code
  featuresScramble = tf.map_fn(tf.random.shuffle,features)

  loss_object = lossFn
  if freeLaminar:
    loss_object = lossFnLaminarCorrected
  if realSpaceLossFn:
    loss_object = lossOnIFFT

  # GradientTape keeps track of gradients so we can backprop
  with tf.GradientTape() as tape:
    predictions = model(featuresScramble, training=True)
    loss = loss_object(labels, predictions)
  gradients = tape.gradient(loss, model.trainable_variables)
  optimizer.apply_gradients(zip(gradients, model.trainable_variables))

  train_loss(loss)

@tf.function
def test_step(images, labels,orig):
  # training=False disables Dropout layer
  predictions = model(images, training=False)

  # Switch between faster default loss function and slower real-space losses
  fastTest = True
  if fastTest:
    t_loss = lossFn(labels, predictions)
  else:
    bs = tf.shape(labels)[0]
    t_loss = tf.keras.losses.mse(tfUnLabelFast(bs,labels),orig)

  test_loss(t_loss)

checkpoint_path = "training_data/cp-{epoch:04d}.ckpt"
checkpoint_dir = os.path.dirname(checkpoint_path)

# Save the epoch zero weights to ensure the folder is populated
if not os.path.isdir(checkpoint_dir):
  model.save_weights(checkpoint_path.format(epoch=0))

latest = tf.train.latest_checkpoint(checkpoint_dir)
model.load_weights(latest)

if meanFlowCorrection:
  visualize(meanFlowFFT,meanFlowFFT,tf.zeros_like(psiOuts[0]),lab="meanFlow",epc="-1")

for epoch in range(EPOCHS):
  # Reset the metrics at the start of the next epoch
  train_loss.reset_states()
  test_loss.reset_states()

  for images, labels, original in train_ds:
    # Progess bar
    print(".",end="",flush=True)
    train_step(images, labels)

  print()

  # How many examples to output as graphs every vizEpochs
  n = 8
  vizEpochs = 10
  vized = False
  for test_features, test_labels, test_original in test_ds:
    print(("'" if vized else "\""),end="",flush=True)
    test_step(test_features, test_labels, test_original)
    recovered = model(test_features,training=False)
    # Also vizualize on last epoch
    if not vized and ((epoch % vizEpochs == 0) or (epoch == EPOCHS - 1)):
      vized = True
      for k in range(n):
        # If this is a truncated epoch, don't crash
        if k < len(test_labels):
          visualize(test_labels[k],recovered[k],test_original[k],str(k),str(epoch))
          # Save the weights everytime we visualize anything, so we can always get that network back
          model.save_weights(checkpoint_path.format(epoch=epoch))
  print()
  print(
    f'Epoch {epoch}, '
    f'Mean Loss: {train_loss.result()}, '
    f'Test Mean Loss: {test_loss.result()}'
  )

# Still save weights even if n=0 graphs
model.save_weights(checkpoint_path.format(epoch=EPOCHS))
